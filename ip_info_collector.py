import os
import configparser
import requests
import pandas as pd
from tqdm import tqdm
import time
import concurrent.futures
import logging
import socket
import subprocess
import platform
import threading
import glob
from collections import defaultdict

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('ip_info_collector.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# è¯»å–é…ç½®æ–‡ä»¶
config = configparser.ConfigParser()
try:
    with open('config.ini', 'r', encoding='utf-8') as f:
        config.read_file(f)
except FileNotFoundError:
    logger.error("é…ç½®æ–‡ä»¶ config.ini ä¸å­˜åœ¨")
    exit(1)
except Exception as e:
    logger.error(f"è¯»å–é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    exit(1)

def get_config_value(section, option, default=None):
    """å®‰å…¨è·å–é…ç½®å€¼"""
    try:
        return config.get(section, option)
    except (configparser.NoSectionError, configparser.NoOptionError):
        return default

# ä»é…ç½®è·å–å¸¸é‡
INPUT_DIR = get_config_value('directories', 'input_dir', './ips/{port}/')
OUTPUT_DIR = get_config_value('directories', 'output_dir', './ip-info/{port}/')
BATCH_SIZE = int(get_config_value('api', 'batch_size', 100))
MAX_RETRIES = int(get_config_value('api', 'max_retries', 3))
MAX_WORKERS = int(get_config_value('api', 'max_workers', 5))
COUNTRIES = [c.strip() for c in get_config_value('processing', 'countries', 'HK,JP,KR,SG,US').split(',')]
MAX_RECORDS = int(get_config_value('processing', 'max_records_per_country', 10))
MARKER_SYMBOL = get_config_value('processing', 'marker_symbol', 'ğŸ¬')

# IPæ£€æµ‹ç›¸å…³é…ç½®
ENABLE_IP_CHECK = get_config_value('ip_check', 'enable_ip_check', 'true').lower() == 'true'
CHECK_TIMEOUT = float(get_config_value('ip_check', 'check_timeout', 2))
CHECK_METHOD = get_config_value('ip_check', 'check_method', 'port')
CHECK_PORT = int(get_config_value('ip_check', 'check_port', 443))
CHECK_THREADS = int(get_config_value('ip_check', 'check_threads', 50))

# åˆå¹¶é…ç½®
PORTS_TO_MERGE = [p.strip() for p in get_config_value('merging', 'ports_to_merge', '443,8443,2053,2083,2087,2096').split(',')]
MERGE_OUTPUT_DIR = get_config_value('merging', 'merge_output_dir', './ip-info/mrked/')
MAX_RECORDS_PER_MERGE = int(get_config_value('merging', 'max_records_per_merge', 10))

# APIé€Ÿç‡é™åˆ¶ç›¸å…³
API_RATE_LIMIT = 15  # ip-api.comæ¯åˆ†é’Ÿæœ€å¤š15ä¸ªè¯·æ±‚
API_WINDOW = 60      # 60ç§’çª—å£

def get_ip_from_file(filename):
    """ä»æ–‡æœ¬æ–‡ä»¶è¯»å–IPåœ°å€"""
    try:
        with open(filename, "r", encoding="utf-8") as f:
            ips = [line.strip() for line in f if line.strip()]
        logger.info(f"ä» {filename} è¯»å–äº† {len(ips)} ä¸ªIP")
        return ips
    except Exception as e:
        logger.error(f"è¯»å–æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
        return []

def ipinfoapi_batch(ips: list, session, retries=MAX_RETRIES):
    """æ‰¹é‡æŸ¥è¯¢IPä¿¡æ¯ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    if not ips:
        return []
        
    url = 'http://ip-api.com/batch'
    ips_dict = [{'query': ip, "fields": "city,country,countryCode,isp,org,as,query"} for ip in ips]
    
    for attempt in range(retries):
        try:
            response = session.post(url, json=ips_dict)
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 429:  # é€Ÿç‡é™åˆ¶
                wait_time = min(60, 10 * (attempt + 1))  # æŒ‡æ•°é€€é¿
                logger.warning(f"APIé€Ÿç‡å—é™ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                logger.error(f'è·å–IPä¿¡æ¯å¤±è´¥: {response.status_code}, {response.reason}')
                return []
        except Exception as e:
            logger.error(f'è¯·æ±‚é”™è¯¯: {e}')
            time.sleep(2)
    
    logger.error(f"ç»è¿‡ {retries} æ¬¡é‡è¯•åä»å¤±è´¥")
    return []

def get_ip_info(ips):
    """è·å–IPä¿¡æ¯ï¼ˆå¸¦é€Ÿç‡é™åˆ¶æ§åˆ¶ï¼‰"""
    if not ips:
        logger.warning("IPåˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡ä¿¡æ¯è·å–")
        return []
        
    # åˆ›å»ºæ‰¹æ¬¡
    batches = [ips[i:i + BATCH_SIZE] for i in range(0, len(ips), BATCH_SIZE)]
    results = []
    
    # è®¡ç®—æœ€å°è¯·æ±‚é—´éš”ï¼ˆéµå®ˆAPIé€Ÿç‡é™åˆ¶ï¼‰
    min_interval = API_WINDOW / API_RATE_LIMIT
    logger.info(f"APIé€Ÿç‡é™åˆ¶: {API_RATE_LIMIT} è¯·æ±‚/åˆ†é’Ÿ, æœ€å°é—´éš”: {min_interval:.2f} ç§’")
    
    with tqdm(total=len(batches), desc="å¤„ç†IPæ‰¹æ¬¡") as pbar:
        with requests.Session() as session:
            last_request_time = 0
            
            for batch in batches:
                # ç¡®ä¿éµå®ˆAPIé€Ÿç‡é™åˆ¶
                current_time = time.time()
                elapsed = current_time - last_request_time
                
                if elapsed < min_interval:
                    wait_time = min_interval - elapsed
                    logger.debug(f"ç­‰å¾… {wait_time:.2f} ç§’ä»¥éµå®ˆAPIé€Ÿç‡é™åˆ¶")
                    time.sleep(wait_time)
                
                # è®°å½•è¯·æ±‚æ—¶é—´
                last_request_time = time.time()
                
                # å¤„ç†æ‰¹æ¬¡
                batch_result = ipinfoapi_batch(batch, session)
                if batch_result:
                    results.extend(batch_result)
                
                pbar.update(1)
    
    return results

def gather_ip_addresses(port):
    """æ”¶é›†æŒ‡å®šç«¯å£çš„IPåœ°å€"""
    port_dir = INPUT_DIR.format(port=port)
    
    if not os.path.exists(port_dir):
        logger.warning(f"ç«¯å£ {port} ç›®å½•ä¸å­˜åœ¨: {port_dir}")
        return []
    
    logger.info(f"æ‰«æç«¯å£ {port} ç›®å½•: {port_dir}")
    all_ips = []
    
    for file in os.listdir(port_dir):
        if file.endswith('.txt'):
            file_path = os.path.join(port_dir, file)
            all_ips.extend(get_ip_from_file(file_path))
    
    unique_ips = list(set(all_ips))
    logger.info(f"æ”¶é›†åˆ° {len(all_ips)} ä¸ªIPåœ°å€ï¼Œå»é‡å: {len(unique_ips)}")
    return unique_ips

def check_ip_port(ip, port, timeout=CHECK_TIMEOUT):
    """æ£€æµ‹IPç«¯å£æ˜¯å¦å¼€æ”¾"""
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.settimeout(timeout)
            result = s.connect_ex((ip, port))
            return result == 0
    except Exception:
        return False

def check_ip_ping(ip, timeout=CHECK_TIMEOUT):
    """æ£€æµ‹IPæ˜¯å¦å¯pingé€š"""
    param = '-n' if platform.system().lower() == 'windows' else '-c'
    command = ['ping', param, '1', '-w', str(int(timeout * 1000)), ip]
    
    try:
        with subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as process:
            _, _ = process.communicate()
            return process.returncode == 0
    except Exception:
        return False

def check_ip(ip):
    """æ ¹æ®é…ç½®æ£€æµ‹IP"""
    if CHECK_METHOD == 'port':
        return check_ip_port(ip, CHECK_PORT, CHECK_TIMEOUT)
    elif CHECK_METHOD == 'ping':
        return check_ip_ping(ip, CHECK_TIMEOUT)
    else:
        logger.warning(f"æœªçŸ¥çš„æ£€æµ‹æ–¹æ³•: {CHECK_METHOD}, é»˜è®¤ä½¿ç”¨ç«¯å£æ£€æµ‹")
        return check_ip_port(ip, CHECK_PORT, CHECK_TIMEOUT)

def check_ips(ips):
    """æ‰¹é‡æ£€æµ‹IPå¯ç”¨æ€§"""
    if not ENABLE_IP_CHECK:
        logger.info("IPæ£€æµ‹å·²ç¦ç”¨ï¼Œè·³è¿‡æ£€æµ‹")
        return [True] * len(ips)
    
    logger.info(f"å¼€å§‹æ£€æµ‹ {len(ips)} ä¸ªIPçš„å¯ç”¨æ€§ (æ–¹æ³•: {CHECK_METHOD})")
    
    valid_ips = []
    lock = threading.Lock()
    
    def check_and_record(ip):
        result = check_ip(ip)
        with lock:
            valid_ips.append((ip, result))
    
    with tqdm(total=len(ips), desc="æ£€æµ‹IPå¯ç”¨æ€§") as pbar:
        with concurrent.futures.ThreadPoolExecutor(max_workers=CHECK_THREADS) as executor:
            futures = {executor.submit(check_and_record, ip): ip for ip in ips}
            
            for future in concurrent.futures.as_completed(futures):
                pbar.update(1)
    
    # æŒ‰åŸå§‹é¡ºåºè¿”å›ç»“æœ
    result_map = {ip: result for ip, result in valid_ips}
    return [result_map.get(ip, False) for ip in ips]

def process_ip_info(ip_info, port):
    """å¤„ç†IPä¿¡æ¯å¹¶æŒ‰å›½å®¶ä¿å­˜"""
    if not ip_info:
        logger.warning(f"æ²¡æœ‰è·å–åˆ°IPä¿¡æ¯ï¼Œè·³è¿‡å¤„ç†ç«¯å£ {port}")
        return
        
    save_dir = OUTPUT_DIR.format(port=port)
    os.makedirs(save_dir, exist_ok=True)

    try:
        df = pd.DataFrame(ip_info)
        
        # æ£€æµ‹IPå¯ç”¨æ€§
        if ENABLE_IP_CHECK:
            ips = df['query'].tolist()
            valid_results = check_ips(ips)
            df['valid'] = valid_results
            logger.info(f"æœ‰æ•ˆIPæ•°é‡: {sum(valid_results)}/{len(valid_results)}")
        else:
            df['valid'] = True
            logger.info("IPæ£€æµ‹å·²ç¦ç”¨ï¼Œè·³è¿‡æœ‰æ•ˆæ€§æ£€æŸ¥")
        
        # åªä¿ç•™æœ‰æ•ˆçš„IP
        valid_df = df[df['valid']]
        
        grouped = valid_df.groupby('countryCode')
        
        for country_code, group in grouped:
            unique_ips = group['query'].drop_duplicates()
            output_file = os.path.join(save_dir, f"{country_code}.txt")
            unique_ips.to_csv(output_file, header=None, index=False)
            logger.info(f"ä¿å­˜ {country_code} çš„ {len(unique_ips)} ä¸ªæœ‰æ•ˆIPåˆ° {output_file}")
            
            # ä¿å­˜åŒ…å«æ‰€æœ‰ä¿¡æ¯çš„CSVæ–‡ä»¶
            info_file = os.path.join(save_dir, f"{country_code}_info.csv")
            group.to_csv(info_file, index=False)
            logger.info(f"ä¿å­˜ {country_code} çš„è¯¦ç»†ä¿¡æ¯åˆ° {info_file}")
    except Exception as e:
        logger.error(f"å¤„ç†IPä¿¡æ¯æ—¶å‡ºé”™: {e}")
        if ip_info:
            logger.debug(f"æ•°æ®ç»“æ„ç¤ºä¾‹: {ip_info[:1]}")

def create_country_marker_files(port):
    """ä¸ºæŒ‡å®šå›½å®¶åˆ›å»ºå¸¦æ ‡è®°çš„æ–‡ä»¶ï¼ˆåœ¨IPåæ·»åŠ ç«¯å£å·ï¼‰"""
    base_dir = OUTPUT_DIR.format(port=port)
    
    for country in COUNTRIES:
        source_path = os.path.join(base_dir, f"{country}.txt")
        target_path = os.path.join(base_dir, f"{country}_marked.txt")
        
        if not os.path.exists(source_path):
            logger.warning(f"å›½å®¶æ–‡ä»¶ä¸å­˜åœ¨: {source_path}")
            continue
            
        try:
            # è¯»å–æºæ–‡ä»¶ï¼Œåªå–å‰MAX_RECORDSè¡Œ
            with open(source_path, "r", encoding="utf-8") as src:
                lines = [line.strip() for line in src.readlines()[:MAX_RECORDS]]
            
            # å†™å…¥ç›®æ ‡æ–‡ä»¶ï¼Œæ·»åŠ ç«¯å£å·å’Œæ ‡è®°
            with open(target_path, "w", encoding="utf-8") as tgt:
                for line in lines:
                    if line:  # è·³è¿‡ç©ºè¡Œ
                        # åœ¨IPåœ°å€åæ·»åŠ ç«¯å£å·å’Œæ ‡è®°
                        tgt.write(f"{line}:{port}#{country}{MARKER_SYMBOL}\n")
            
            logger.info(f"å·²åˆ›å»ºæ ‡è®°æ–‡ä»¶: {target_path} (åŒ…å« {len(lines)} æ¡è®°å½•ï¼Œç«¯å£: {port})")
        except Exception as e:
            logger.error(f"å¤„ç† {country} æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def merge_marked_files():
    """åˆå¹¶æ‰€æœ‰ç«¯å£çš„æ ‡è®°æ–‡ä»¶åˆ°æŒ‡å®šç›®å½•ï¼ˆæ–°å¢çš„æ–‡ä»¶åˆå¹¶åŠŸèƒ½ï¼‰"""
    logger.info("å¼€å§‹åˆå¹¶æ ‡è®°æ–‡ä»¶")
    
    # åˆ›å»ºåˆå¹¶è¾“å‡ºç›®å½•
    os.makedirs(MERGE_OUTPUT_DIR, exist_ok=True)
    
    # ä½¿ç”¨å­—å…¸å­˜å‚¨æ¯ä¸ªå›½å®¶çš„æ‰€æœ‰è®°å½•
    country_records = defaultdict(list)
    total_files_found = 0
    
    # æ”¶é›†æ‰€æœ‰ç«¯å£çš„æ ‡è®°æ–‡ä»¶å†…å®¹
    for port in PORTS_TO_MERGE:
        port_dir = OUTPUT_DIR.format(port=port)
        
        if not os.path.exists(port_dir):
            logger.warning(f"ç«¯å£ç›®å½•ä¸å­˜åœ¨: {port_dir}")
            continue
            
        for country in COUNTRIES:
            marked_file_path = os.path.join(port_dir, f"{country}_marked.txt")
            
            if os.path.exists(marked_file_path):
                try:
                    with open(marked_file_path, "r", encoding="utf-8") as f:
                        lines = [line.strip() for line in f if line.strip()]
                        country_records[country].extend(lines)
                    total_files_found += 1
                    logger.info(f"ä»ç«¯å£ {port} è¯»å–äº† {len(lines)} æ¡ {country} è®°å½•")
                except Exception as e:
                    logger.error(f"è¯»å–æ–‡ä»¶ {marked_file_path} æ—¶å‡ºé”™: {e}")
    
    logger.info(f"æ€»å…±æ‰¾åˆ° {total_files_found} ä¸ªæ ‡è®°æ–‡ä»¶ï¼Œæ¶‰åŠ {len(country_records)} ä¸ªå›½å®¶")
    
    # ä¸ºæ¯ä¸ªå›½å®¶åˆ›å»ºåˆå¹¶æ–‡ä»¶
    merged_count = 0
    for country, records in country_records.items():
        if not records:
            logger.warning(f"å›½å®¶ {country} æ²¡æœ‰è®°å½•ï¼Œè·³è¿‡")
            continue
            
        # å»é‡å¹¶é™åˆ¶è®°å½•æ•°é‡
        unique_records = list(set(records))
        if len(unique_records) > MAX_RECORDS_PER_MERGE:
            unique_records = unique_records[:MAX_RECORDS_PER_MERGE]
            logger.info(f"å›½å®¶ {country} è®°å½•æ•°è¶…è¿‡é™åˆ¶ï¼Œä¿ç•™å‰ {MAX_RECORDS_PER_MERGE} æ¡")
        
        # å†™å…¥åˆå¹¶æ–‡ä»¶
        output_file = os.path.join(MERGE_OUTPUT_DIR, f"{country}2_mrked.txt")
        try:
            with open(output_file, "w", encoding="utf-8") as f:
                for record in unique_records:
                    f.write(f"{record}\n")
            
            merged_count += 1
            logger.info(f"åˆå¹¶å®Œæˆ: {output_file} (åŒ…å« {len(unique_records)} æ¡å”¯ä¸€è®°å½•)")
        except Exception as e:
            logger.error(f"å†™å…¥åˆå¹¶æ–‡ä»¶ {output_file} æ—¶å‡ºé”™: {e}")
    
    logger.info(f"æ ‡è®°æ–‡ä»¶åˆå¹¶å®Œæˆï¼Œå…±ç”Ÿæˆ {merged_count} ä¸ªåˆå¹¶æ–‡ä»¶")

def process_single_port(port):
    """å¤„ç†å•ä¸ªç«¯å£çš„å®Œæ•´æµç¨‹"""
    logger.info(f"å¼€å§‹å¤„ç†ç«¯å£ {port}")
    
    # æ”¶é›†IPåœ°å€
    ips = gather_ip_addresses(port)
    
    if not ips:
        logger.warning(f"ç«¯å£ {port} æ²¡æœ‰æ‰¾åˆ°IPåœ°å€ï¼Œè·³è¿‡APIè°ƒç”¨")
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ï¼Œå³ä½¿æ²¡æœ‰æ•°æ®ä¹Ÿåˆ›å»ºç›®å½•ç»“æ„
        save_dir = OUTPUT_DIR.format(port=port)
        os.makedirs(save_dir, exist_ok=True)
        return
    
    # è·å–IPä¿¡æ¯
    ip_info = get_ip_info(ips)
    
    # å¤„ç†IPä¿¡æ¯å¹¶ä¿å­˜
    process_ip_info(ip_info, port)
    
    # åˆ›å»ºæ ‡è®°æ–‡ä»¶
    create_country_marker_files(port)
    
    logger.info(f"ç«¯å£ {port} å¤„ç†å®Œæˆ")

def main(ports_config):
    """ä¸»å¤„ç†å‡½æ•°ï¼Œæ”¯æŒå¤šç«¯å£"""
    if isinstance(ports_config, str):
        ports = [p.strip() for p in ports_config.split(',')]
    else:
        ports = [ports_config] if ports_config else ['443']  # é»˜è®¤ç«¯å£
    
    logger.info(f"é…ç½®å¤„ç†çš„ç«¯å£åˆ—è¡¨: {ports}")
    
    # å¾ªç¯å¤„ç†æ¯ä¸ªç«¯å£
    for port in ports:
        try:
            process_single_port(port)
        except Exception as e:
            logger.error(f"å¤„ç†ç«¯å£ {port} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue
    
    # åˆå¹¶æ‰€æœ‰æ ‡è®°æ–‡ä»¶
    merge_marked_files()
    
    logger.info(f"æ‰€æœ‰ç«¯å£å¤„ç†å®Œæˆ: {ports}")

if __name__ == "__main__":
    # ä»é…ç½®è·å–è¦å¤„ç†çš„ç«¯å£
    ports_config = get_config_value('settings', 'ports', '443')
    
    logger.info("=" * 50)
    logger.info(f"IPä¿¡æ¯æ”¶é›†å™¨å¯åŠ¨")
    logger.info(f"é…ç½®å‚æ•°:")
    logger.info(f"  å¤„ç†ç«¯å£: {ports_config}")
    logger.info(f"  åˆå¹¶ç«¯å£: {', '.join(PORTS_TO_MERGE)}")
    logger.info(f"  åˆå¹¶è¾“å‡ºç›®å½•: {MERGE_OUTPUT_DIR}")
    logger.info(f"  æ¯ä¸ªåˆå¹¶æ–‡ä»¶æœ€å¤§è®°å½•æ•°: {MAX_RECORDS_PER_MERGE}")
    logger.info("=" * 50)
    
    try:
        main(ports_config)
        logger.info("æ‰€æœ‰å¤„ç†å®Œæˆ!")
    except Exception as e:
        logger.exception("å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯")